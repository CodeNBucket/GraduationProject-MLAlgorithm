import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,fbeta_score
import pandas as pd
from gensim.models.doc2vec import Doc2Vec,TaggedDocument
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
import gensim
from sklearn.linear_model import LogisticRegression
import random
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
file_path = r"C:\Users\Turgut\Desktop\Dataset\output_review_yelpHotelData_NRYRcleaned.txt"
text_file=open(file_path)
reviews=text_file.readlines()#reviews contains all the text information
text_file.close()
file_path=r"C:\Users\Turgut\Desktop\Dataset\output_meta_yelpHotelData_NRYRcleaned.txt"
meta_file=open(file_path)
meta=meta_file.readlines()
meta_file.close()
labels=[]
for label in meta:
    labels.append(label.split()[4])
#X_train,X_test, y_train, y_test = train_test_split( reviews, labels, random_state=0)

def read_corpus(f): #Taggs the documents

        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            yield TaggedDocument(tokens, [i])

train_corpus = list(read_corpus(reviews)) #Training set with tagged document(.tags,.words)
model=Doc2Vec.load(r"C:\Users\Turgut\Desktop\Dataset\trained_model")


ranks = []
for doc_id in range(len(train_corpus)):
    inferred_vector = model.infer_vector(train_corpus[doc_id].words) #takes the inferred vectors one by one
    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv)) #compares it to all other documents and list the docs most similar to least similar (doc_id,similarityscore)
    rank = [docid for docid, sim in sims].index(doc_id)# finds the location of doc_id and stores it in the rank(if the inferred_vector is the most similar doc_id will be at the 1'st place(index[0]) which means it is working well)
    ranks.append(rank)#stores them in rank


dict={}
for i in ranks:
    if i==0:
        if 0 in dict.keys():
            dict[0]+=1
        else:
            dict[0]=1
    else:
        if 1 in dict.keys():
            dict[1] += 1
        else:
            dict[1] = 1

print(dict)#for better idea

vectors=[]
for i in range(len(train_corpus)):
    vector=model.dv[i]
    vectors.append(vector)#vectors contains vector embeddings of dataset

X_train,X_test, y_train, y_test = train_test_split( vectors, labels, random_state=0)



C_values=[0.001,0.01,0.1,1,10,100,1000]
for C in C_values:
    LogisticRegressionModel = LogisticRegression(class_weight={'N': 1, 'Y': 6},C=C,max_iter=2000)
    LogisticRegressionModel.fit(X_train, y_train)
    y_pred=LogisticRegressionModel.predict(X_test)
    y_true=y_test
    print(y_pred[10:30])
    print(y_pred[240:270])
    print("Test set score: {:.2f}".format(LogisticRegressionModel.score(X_test, y_test)))
    print(C)
    # Accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print("Accuracy:", accuracy)

    # Precision
    precision = precision_score(y_true, y_pred,pos_label='N')
    print("Precision:", precision)

    # Recall
    recall = recall_score(y_true, y_pred,pos_label='N')
    print("Recall:", recall)

    # F1-Score
    f1 = f1_score(y_true, y_pred,pos_label='N')
    print("F1-Score:", f1)
    fb = fbeta_score(y_true, y_pred, pos_label='N', beta=0.5)
    print("Fbeta-Score:", fb)
    label_encoder = LabelEncoder()
    y_true_binary = label_encoder.fit_transform(y_true)
    y_pred_binary = label_encoder.fit_transform(y_pred)


    # ROC-AUC
    roc_auc = roc_auc_score(y_true_binary, y_pred_binary)
    print("ROC-AUC:", roc_auc)
