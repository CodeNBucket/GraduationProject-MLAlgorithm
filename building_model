import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from gensim.models.doc2vec import Doc2Vec,TaggedDocument
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
import gensim
from sklearn.linear_model import LogisticRegression
import random
file_path = r"C:\Users\Turgut\Desktop\Dataset\output_review_yelpHotelData_NRYRcleaned.txt"
text_file=open(file_path)
reviews=text_file.readlines()#reviews contains all the text information
text_file.close()
file_path=r"C:\Users\Turgut\Desktop\Dataset\output_meta_yelpHotelData_NRYRcleaned.txt"
meta_file=open(file_path)
meta=meta_file.readlines()
meta_file.close()
labels=[]
for label in meta:
    labels.append(label.split()[4])
#X_train,X_test, y_train, y_test = train_test_split( reviews, labels, random_state=0)

def read_corpus(f): #Taggs the documents

        for i, line in enumerate(f):
            tokens = gensim.utils.simple_preprocess(line)
            yield TaggedDocument(tokens, [i])

train_corpus = list(read_corpus(reviews)) #Training set with tagged document(.tags,.words)
model = Doc2Vec(vector_size=384, min_count=1, epochs=10,window=10)
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)
model.save(r"C:\Users\Turgut\Desktop\Dataset\trained_model")
